<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 750px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"abstract": "Since its introduction two decades ago, there has been increasing interest in the problem of early classification of time series. This problem generalizes classic time series classification to ask if we can classify a time series subsequence with sufficient accuracy and confidence after seeing only some prefix of a target pattern. The idea is that the earlier classification would allow us to take immediate action, in a domain in which some practical interventions are possible. For example, that intervention might be sounding an alarm or applying the brakes in an automobile. In this work, we make a surprising claim. In spite of the fact that there are dozens of papers on early classification of time series, it is not clear that any of them could ever work in a real-world setting. The problem is not with the algorithms per se but with the vague and underspecified problem description. Essentially all algorithms make implicit and unwarranted assumptions about the problem that will ensure that they will be plagued by false positives and false negatives even if their results suggested that they could obtain near-perfect results. We will explain our findings with novel insights and experiments and offer recommendations to the community.", "color": "#ff7f0e", "id": "b539183ab4837cede4ff91180bbce0bc56a2a635", "label": "b539183ab4837cede4ff91180bbce0bc56a2a635", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e b539183ab4837cede4ff91180bbce0bc56a2a635\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e When is Early Classification of Time Series Meaningful?\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eSince its introduction two decades ago, there has been increasing interest in the problem of early classification of time series. This problem generalizes classic time series classification to ask if we can classify a time series subsequence with sufficient accuracy and confidence after seeing only some prefix of a target pattern. The idea is that the earlier classification would allow us to take immediate action, in a domain in which some practical interventions are possible. For example, that intervention might be sounding an alarm or applying the brakes in an automobile. In this work, we make a surprising claim. In spite of the fact that there are dozens of papers on early classification of time series, it is not clear that any of them could ever work in a real-world setting. The problem is not with the algorithms per se but with the vague and underspecified problem description. Essentially all algorithms make implicit and unwarranted assumptions about the problem that will ensure that they will be plagued by false positives and false negatives even if their results suggested that they could obtain near-perfect results. We will explain our findings with novel insights and experiments and offer recommendations to the community.\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "label": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 0b72c321cd990df3b8960fc68e482d2f9e1834ab\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2015.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time series classification with ensembles of elastic distance measures\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "The similarity measure is a fundamental and key problem for time series analysis, which is widely used in classification, cluster, motif discovery and etc. Time series similarity calculation has been studied a lot and many methods have been proposed, such as Euclidean distance and elastic distance measures. However, all these methods are distance-based which get the similarity by accumulating the distances between optimal match pairs and ignore the intrinsic differences of time series. In order to solve the drawback, we discard this distance-based technique and design a Siamese Convolutional Neural Network(SCNN) to obtain the similarity of time series in this paper. Concretely, we first extract the essential features for similarity calculation by Convolutional Neural Network and then ontain the absolute difference between the essential features of time series. To conveniently train the proposed model, we make full use of the label information of datasets and construct a new binary dataset which each example contains two original time series and a binary label. In the proposed Siamese network, the parameters are shared between two branches and we take binary cross-entropy function to train our model. Experimental results on synthetic and real time series datasets show that our SCNN model is effective and achieves improved accuracy for time series classification task compared to other time series similarity calculation methods.", "color": "#aec7e8", "id": "db210fc464d741f3f627190711ec0150d983f280", "label": "db210fc464d741f3f627190711ec0150d983f280", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e db210fc464d741f3f627190711ec0150d983f280\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2019.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time Series Similarity Measure via Siamese Convolutional Neural Network\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThe similarity measure is a fundamental and key problem for time series analysis, which is widely used in classification, cluster, motif discovery and etc. Time series similarity calculation has been studied a lot and many methods have been proposed, such as Euclidean distance and elastic distance measures. However, all these methods are distance-based which get the similarity by accumulating the distances between optimal match pairs and ignore the intrinsic differences of time series. In order to solve the drawback, we discard this distance-based technique and design a Siamese Convolutional Neural Network(SCNN) to obtain the similarity of time series in this paper. Concretely, we first extract the essential features for similarity calculation by Convolutional Neural Network and then ontain the absolute difference between the essential features of time series. To conveniently train the proposed model, we make full use of the label information of datasets and construct a new binary dataset which each example contains two original time series and a binary label. In the proposed Siamese network, the parameters are shared between two branches and we take binary cross-entropy function to train our model. Experimental results on synthetic and real time series datasets show that our SCNN model is effective and achieves improved accuracy for time series classification task compared to other time series similarity calculation methods.\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "0326a8edf41d09beb171ca306b1de95d8d85857e", "label": "0326a8edf41d09beb171ca306b1de95d8d85857e", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 0326a8edf41d09beb171ca306b1de95d8d85857e\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2013.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time Series Classification Using Compression Distance of Recurrence Plots\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "We present a novel framework for multivariate time series representation learning based on the transformer encoder architecture. The framework includes an unsupervised pre-training scheme, which can offer substantial performance benefits over fully supervised learning on downstream tasks, both with but even without leveraging additional unlabeled data, i.e., by reusing the existing data samples. Evaluating our framework on several public multivariate time series datasets from various domains and with diverse characteristics, we demonstrate that it performs significantly better than the best currently available methods for regression and classification, even for datasets which consist of only a few hundred training samples. Given the pronounced interest in unsupervised learning for nearly all domains in the sciences and in industry, these findings represent an important landmark, presenting the first unsupervised method shown to push the limits of state-of-the-art performance for multivariate time series regression and classification.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "2051548f7681c96d603de932ee23406c525276f9", "label": "2051548f7681c96d603de932ee23406c525276f9", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 2051548f7681c96d603de932ee23406c525276f9\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2020.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e A Transformer-based Framework for Multivariate Time Series Representation Learning\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eWe present a novel framework for multivariate time series representation learning based on the transformer encoder architecture. The framework includes an unsupervised pre-training scheme, which can offer substantial performance benefits over fully supervised learning on downstream tasks, both with but even without leveraging additional unlabeled data, i.e., by reusing the existing data samples. Evaluating our framework on several public multivariate time series datasets from various domains and with diverse characteristics, we demonstrate that it performs significantly better than the best currently available methods for regression and classification, even for datasets which consist of only a few hundred training samples. Given the pronounced interest in unsupervised learning for nearly all domains in the sciences and in industry, these findings represent an important landmark, presenting the first unsupervised method shown to push the limits of state-of-the-art performance for multivariate time series regression and classification.\u003c/div\u003e"}, {"abstract": "Multivariate time series classification has become popular due to its prevalence in many real-world applications. However, most state-of-the-art focuses on improving classification performance, with the best-performing models typically opaque. Interpretable multivariate time series classifiers have been recently introduced, but none can maintain sufficient levels of efficiency and effectiveness together with interpretability. We introduce Z-Time , a novel algorithm for effective and efficient interpretable multivariate time series classification. Z-Time employs temporal abstraction and temporal relations of event intervals to create interpretable features across multiple time series dimensions. In our experimental evaluation on the UEA multivariate time series datasets, Z-Time achieves comparable effectiveness to state-of-the-art non-interpretable multivariate classifiers while being faster than all interpretable multivariate classifiers. We also demonstrate that Z-Time is more robust to missing values and inter-dimensional orders, compared to its interpretable competitors.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "label": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 83bc34cf87d07f53b803c878b0e0f78fe6326de1\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Z-Time: efficient and effective interpretable multivariate time series classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMultivariate time series classification has become popular due to its prevalence in many real-world applications. However, most state-of-the-art focuses on improving classification performance, with the best-performing models typically opaque. Interpretable multivariate time series classifiers have been recently introduced, but none can maintain sufficient levels of efficiency and effectiveness together with interpretability. We introduce Z-Time , a novel algorithm for effective and efficient interpretable multivariate time series classification. Z-Time employs temporal abstraction and temporal relations of event intervals to create interpretable features across multiple time series dimensions. In our experimental evaluation on the UEA multivariate time series datasets, Z-Time achieves comparable effectiveness to state-of-the-art non-interpretable multivariate classifiers while being faster than all interpretable multivariate classifiers. We also demonstrate that Z-Time is more robust to missing values and inter-dimensional orders, compared to its interpretable competitors.\u003c/div\u003e"}, {"abstract": "", "color": "#ff7f0e", "id": "fb25011c564139e37e6b912edfff8d61a04893f4", "label": "fb25011c564139e37e6b912edfff8d61a04893f4", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e fb25011c564139e37e6b912edfff8d61a04893f4\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e PETSC: pattern-based embedding for time series classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "Rocket achieves state-of-the-art accuracy for time series classification with a fraction of the computational expense of most existing methods by transforming input time series using random convolutional kernels, and using the transformed features to train a linear classifier. We reformulate Rocket into a new method, MiniRocket. MiniRocket is up to 75 times faster than Rocket on larger datasets, and almost deterministic (and optionally, fully deterministic), while maintaining essentially the same accuracy. Using this method, it is possible to train and test a classifier on all of 109 datasets from the UCR archive to state-of-the-art accuracy in under 10 minutes. MiniRocket is significantly faster than any other method of comparable accuracy (including Rocket), and significantly more accurate than any other method of remotely similar computational expense.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "label": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e bbae866c5edd477c1c39921436c47fd43d1f6e5c\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2020.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eRocket achieves state-of-the-art accuracy for time series classification with a fraction of the computational expense of most existing methods by transforming input time series using random convolutional kernels, and using the transformed features to train a linear classifier. We reformulate Rocket into a new method, MiniRocket. MiniRocket is up to 75 times faster than Rocket on larger datasets, and almost deterministic (and optionally, fully deterministic), while maintaining essentially the same accuracy. Using this method, it is possible to train and test a classifier on all of 109 datasets from the UCR archive to state-of-the-art accuracy in under 10 minutes. MiniRocket is significantly faster than any other method of comparable accuracy (including Rocket), and significantly more accurate than any other method of remotely similar computational expense.\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "be9892e207007d5f4c3e23133ea03daef64b87b0", "label": "be9892e207007d5f4c3e23133ea03daef64b87b0", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e be9892e207007d5f4c3e23133ea03daef64b87b0\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2015.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time-Series Classification with COTE: The Collective of Transformation-Based Ensembles\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "Accuracy is a key focus of current work in time series classification. However, speed and data reduction are equally important in many applications, especially when the data scale and storage requirements rapidly increase. Current multivariate time series classification (MTSC) algorithms need hundreds of compute hours to complete training and prediction. This is due to the nature of multivariate time series data which grows with the number of time series, their length and the number of channels. In many applications, not all the channels are useful for the classification task, hence we require methods that can efficiently select useful channels and thus save computational resources. We propose and evaluate two methods for channel selection. Our techniques work by representing each class by a prototype time series and performing channel selection based on the prototype distance between classes. The main hypothesis is that useful channels enable better separation between classes; hence, channels with a larger distance between class prototypes are more useful. On the UEA MTSC benchmark, we show that these techniques achieve significant data reduction and classifier speedup for similar levels of classification accuracy. Channel selection is applied as a pre-processing step before training state-of-the-art MTSC algorithms and saves about 70% of computation time and data storage with preserved accuracy. Furthermore, our methods enable efficient classifiers, such as ROCKET, to achieve better accuracy than using no selection or greedy forward channel selection. To further study the impact of our techniques, we present experiments on classifying synthetic multivariate time series datasets with more than 100 channels, as well as a real-world case study on a dataset with 50 channels. In both cases, our channel selection methods result in significant data reduction with preserved or improved accuracy.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "label": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e ea642fd5aa2a097c249d47ab8687687b42bee6f8\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Scalable classifier-agnostic channel selection for multivariate time series classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eAccuracy is a key focus of current work in time series classification. However, speed and data reduction are equally important in many applications, especially when the data scale and storage requirements rapidly increase. Current multivariate time series classification (MTSC) algorithms need hundreds of compute hours to complete training and prediction. This is due to the nature of multivariate time series data which grows with the number of time series, their length and the number of channels. In many applications, not all the channels are useful for the classification task, hence we require methods that can efficiently select useful channels and thus save computational resources. We propose and evaluate two methods for channel selection. Our techniques work by representing each class by a prototype time series and performing channel selection based on the prototype distance between classes. The main hypothesis is that useful channels enable better separation between classes; hence, channels with a larger distance between class prototypes are more useful. On the UEA MTSC benchmark, we show that these techniques achieve significant data reduction and classifier speedup for similar levels of classification accuracy. Channel selection is applied as a pre-processing step before training state-of-the-art MTSC algorithms and saves about 70% of computation time and data storage with preserved accuracy. Furthermore, our methods enable efficient classifiers, such as ROCKET, to achieve better accuracy than using no selection or greedy forward channel selection. To further study the impact of our techniques, we present experiments on classifying synthetic multivariate time series datasets with more than 100 channels, as well as a real-world case study on a dataset with 50 channels. In both cases, our channel selection methods result in significant data reduction with preserved or improved accuracy.\u003c/div\u003e"}, {"abstract": "A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC. We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.", "color": {"background": "#aec7e8", "border": "#d62728", "highlight": {"background": "#aec7e8", "border": "#d62728"}, "hover": {"background": "#aec7e8", "border": "#d62728"}}, "id": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "label": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 32f215640451bb0a06ac97e04c8f89f3dbeb5e15\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2018.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time Series Classification with HIVE-COTE\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eA recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC. We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.\u003c/div\u003e"}, {"abstract": "Multivariate time series classification is a machine learning problem that can be applied to automate a wide range of real-world data analysis tasks. RandOm Convolutional KErnel Transform (ROCKET) proved to be an outstanding algorithm capable to classify time series accurately and quickly. The textbook variant of the multivariate time series classification problem assumes that time series to be classified are all of the same length, while in real-world applications this assumption not necessarily holds. The literature of this domain does not pay enough attention to data processing pipelines for variable-length time series. Thus, in this paper, we present a thorough analysis of three preprocessing pipelines that handle variable-length time series that need to be classified with a method that requires the data to be of equal length. These three methods are truncation, padding, and forecasting of missing value. Experiments conducted on benchmark datasets, showed that the recommended procedure involves padding. Forecasting ensures similar classification accuracy, but comes at a much higher computational cost. Truncation is not a viable option. Furthermore, in the paper, we present a novel domain of application of multivariate time series classification algorithms, that is incident detection in cash transactions. This area poses substantive challenges for automated model training procedures since the data is not only variable-length, but also heavily imbalanced. In the study, we list various incident types and present trained classifiers capable to aid human auditors in their daily work.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "f2260a53350a5166f757979f435a3c477dec6d3e", "label": "f2260a53350a5166f757979f435a3c477dec6d3e", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e f2260a53350a5166f757979f435a3c477dec6d3e\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Variable-Length Multivariate Time Series Classification Using ROCKET: A Case Study of Incident Detection\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMultivariate time series classification is a machine learning problem that can be applied to automate a wide range of real-world data analysis tasks. RandOm Convolutional KErnel Transform (ROCKET) proved to be an outstanding algorithm capable to classify time series accurately and quickly. The textbook variant of the multivariate time series classification problem assumes that time series to be classified are all of the same length, while in real-world applications this assumption not necessarily holds. The literature of this domain does not pay enough attention to data processing pipelines for variable-length time series. Thus, in this paper, we present a thorough analysis of three preprocessing pipelines that handle variable-length time series that need to be classified with a method that requires the data to be of equal length. These three methods are truncation, padding, and forecasting of missing value. Experiments conducted on benchmark datasets, showed that the recommended procedure involves padding. Forecasting ensures similar classification accuracy, but comes at a much higher computational cost. Truncation is not a viable option. Furthermore, in the paper, we present a novel domain of application of multivariate time series classification algorithms, that is incident detection in cash transactions. This area poses substantive challenges for automated model training procedures since the data is not only variable-length, but also heavily imbalanced. In the study, we list various incident types and present trained classifiers capable to aid human auditors in their daily work.\u003c/div\u003e"}, {"abstract": "Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.", "color": "#aec7e8", "id": "9ae9d2b060e50094be7e473e449f192403019225", "label": "9ae9d2b060e50094be7e473e449f192403019225", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 9ae9d2b060e50094be7e473e449f192403019225\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2017.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Multitask learning and benchmarking with clinical time series data\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eHealth care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.\u003c/div\u003e"}, {"abstract": "Given its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance \u0026quot;treadstone.\u0026quot; In this light, we argue that the intra-sequence non-stationarity and inter-sequence asynchronism are two types of heterogeneities widely existed in multiple times series, and propose a hybrid attention network called WHEN as deep learning solution. WHEN features in two attention mechanisms in two different modules. In the WaveAtt module, we propose a novel data-dependent wavelet function and integrate it into the BiLSTM network as the wavelet attention, for the purpose of analyzing dynamic frequency components in nonstationary time series. In the DTWAtt module, we transform the dynamic time warping (DTW) technique into the form as the DTW attention, where all input sequences are synchronized with a universal parameter sequence to overcome the time distortion problem in multiple time series. WHEN with the hybrid attentions is then formulated as task-dependent neural network for either classification or forecasting tasks. Extensive experiments on 30 UEA datasets and 3 real-world datasets with rich competitive baselines demonstrate the excellent performance of our model. The ability of WHEN in dealing with time series heterogeneities is also elaborately explored via specially designed analysis.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "label": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e c2e57a1926217f67a72c617d09fa12ec8e667d0e\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analysis\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eGiven its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance \u0026quot;treadstone.\u0026quot; In this light, we argue that the intra-sequence non-stationarity and inter-sequence asynchronism are two types of heterogeneities widely existed in multiple times series, and propose a hybrid attention network called WHEN as deep learning solution. WHEN features in two attention mechanisms in two different modules. In the WaveAtt module, we propose a novel data-dependent wavelet function and integrate it into the BiLSTM network as the wavelet attention, for the purpose of analyzing dynamic frequency components in nonstationary time series. In the DTWAtt module, we transform the dynamic time warping (DTW) technique into the form as the DTW attention, where all input sequences are synchronized with a universal parameter sequence to overcome the time distortion problem in multiple time series. WHEN with the hybrid attentions is then formulated as task-dependent neural network for either classification or forecasting tasks. Extensive experiments on 30 UEA datasets and 3 real-world datasets with rich competitive baselines demonstrate the excellent performance of our model. The ability of WHEN in dealing with time series heterogeneities is also elaborately explored via specially designed analysis.\u003c/div\u003e"}, {"abstract": "In the last 5 years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "label": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 3b421dcebe21bf45c1930b948cfda7ec95b55ac4\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2016.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eIn the last 5 years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.\u003c/div\u003e"}, {"abstract": "Background: Digital clinical measures collected via various digital sensing technologies such as smartphones, smartwatches, wearables, and ingestible and implantable sensors are increasingly used by individuals and clinicians to capture the health outcomes or behavioral and physiological characteristics of individuals. Time series classification (TSC) is very commonly used for modeling digital clinical measures. While deep learning models for TSC are very common and powerful, there exist some fundamental challenges. This review presents the non-deep learning models that are commonly used for time series classification in biomedical applications that can achieve high performance. Objective: We performed a systematic review to characterize the techniques that are used in time series classification of digital clinical measures throughout all the stages of data processing and model building. Methods: We conducted a literature search on PubMed, as well as the Institute of Electrical and Electronics Engineers (IEEE), Web of Science, and SCOPUS databases using a range of search terms to retrieve peer-reviewed articles that report on the academic research about digital clinical measures from a five-year period between June 2016 and June 2021. We identified and categorized the research studies based on the types of classification algorithms and sensor input types. Results: We found 452 papers in total from four different databases: PubMed, IEEE, Web of Science Database, and SCOPUS. After removing duplicates and irrelevant papers, 135 articles remained for detailed review and data extraction. Among these, engineered features using time series methods that were subsequently fed into widely used machine learning classifiers were the most commonly used technique, and also most frequently achieved the best performance metrics (77 out of 135 articles). Statistical modeling (24 out of 135 articles) algorithms were the second most common and also the second-best classification technique. Conclusions: In this review paper, summaries of the time series classification models and interpretation methods for biomedical applications are summarized and categorized. While high time series classification performance has been achieved in digital clinical, physiological, or biomedical measures, no standard benchmark datasets, modeling methods, or reporting methodology exist. There is no single widely used method for time series model development or feature interpretation, however many different methods have proven successful.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "884024110a3ae7050bbf3bde9d737e1fc120eed6", "label": "884024110a3ae7050bbf3bde9d737e1fc120eed6", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 884024110a3ae7050bbf3bde9d737e1fc120eed6\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e A Systematic Review of Time Series Classification Techniques Used in Biomedical Applications\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eBackground: Digital clinical measures collected via various digital sensing technologies such as smartphones, smartwatches, wearables, and ingestible and implantable sensors are increasingly used by individuals and clinicians to capture the health outcomes or behavioral and physiological characteristics of individuals. Time series classification (TSC) is very commonly used for modeling digital clinical measures. While deep learning models for TSC are very common and powerful, there exist some fundamental challenges. This review presents the non-deep learning models that are commonly used for time series classification in biomedical applications that can achieve high performance. Objective: We performed a systematic review to characterize the techniques that are used in time series classification of digital clinical measures throughout all the stages of data processing and model building. Methods: We conducted a literature search on PubMed, as well as the Institute of Electrical and Electronics Engineers (IEEE), Web of Science, and SCOPUS databases using a range of search terms to retrieve peer-reviewed articles that report on the academic research about digital clinical measures from a five-year period between June 2016 and June 2021. We identified and categorized the research studies based on the types of classification algorithms and sensor input types. Results: We found 452 papers in total from four different databases: PubMed, IEEE, Web of Science Database, and SCOPUS. After removing duplicates and irrelevant papers, 135 articles remained for detailed review and data extraction. Among these, engineered features using time series methods that were subsequently fed into widely used machine learning classifiers were the most commonly used technique, and also most frequently achieved the best performance metrics (77 out of 135 articles). Statistical modeling (24 out of 135 articles) algorithms were the second most common and also the second-best classification technique. Conclusions: In this review paper, summaries of the time series classification models and interpretation methods for biomedical applications are summarized and categorized. While high time series classification performance has been achieved in digital clinical, physiological, or biomedical measures, no standard benchmark datasets, modeling methods, or reporting methodology exist. There is no single widely used method for time series model development or feature interpretation, however many different methods have proven successful.\u003c/div\u003e"}, {"abstract": "Recent years have witnessed the unprecedented rising of time series from almost all kindes of academic and industrial fields. Various types of deep neural network models have been introduced to time series analysis, but the important frequency information is yet lack of effective modeling. In light of this, in this paper we propose a wavelet-based neural network structure called multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware deep learning models for time series analysis. mWDN preserves the advantage of multilevel discrete wavelet decomposition in frequency learning while enables the fine-tuning of all parameters under a deep neural network framework. Based on mWDN, we further propose two deep learning models called Residual Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for time series classification and forecasting, respectively. The two models take all or partial mWDN decomposed sub-series in different frequencies as input, and resort to the back propagation algorithm to learn all the parameters globally, which enables seamless embedding of wavelet-based frequency analysis into deep learning frameworks. Extensive experiments on 40 UCR datasets and a real-world user volume dataset demonstrate the excellent performance of our time series models based on mWDN. In particular, we propose an importance analysis method to mWDN based models, which successfully identifies those time-series elements and mWDN layers that are crucially important to time series analysis. This indeed indicates the interpretability advantage of mWDN, and can be viewed as an indepth exploration to interpretable deep learning.", "color": "#aec7e8", "id": "939043b58d41ca05ca228ae3081c9a540f770a47", "label": "939043b58d41ca05ca228ae3081c9a540f770a47", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 939043b58d41ca05ca228ae3081c9a540f770a47\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2018.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eRecent years have witnessed the unprecedented rising of time series from almost all kindes of academic and industrial fields. Various types of deep neural network models have been introduced to time series analysis, but the important frequency information is yet lack of effective modeling. In light of this, in this paper we propose a wavelet-based neural network structure called multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware deep learning models for time series analysis. mWDN preserves the advantage of multilevel discrete wavelet decomposition in frequency learning while enables the fine-tuning of all parameters under a deep neural network framework. Based on mWDN, we further propose two deep learning models called Residual Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for time series classification and forecasting, respectively. The two models take all or partial mWDN decomposed sub-series in different frequencies as input, and resort to the back propagation algorithm to learn all the parameters globally, which enables seamless embedding of wavelet-based frequency analysis into deep learning frameworks. Extensive experiments on 40 UCR datasets and a real-world user volume dataset demonstrate the excellent performance of our time series models based on mWDN. In particular, we propose an importance analysis method to mWDN based models, which successfully identifies those time-series elements and mWDN layers that are crucially important to time series analysis. This indeed indicates the interpretability advantage of mWDN, and can be viewed as an indepth exploration to interpretable deep learning.\u003c/div\u003e"}, {"abstract": "Existing approaches to time series classification can be grouped into shape-based (numeric) and structure-based (symbolic). Shape-based techniques use the raw numeric time series with Euclidean or Dynamic Time Warping distance and a 1-Nearest Neighbor classifier. They are accurate, but computationally intensive. Structure-based methods discretize the raw data into symbolic representations, then extract features for classifiers. Recent symbolic methods have outperformed numeric ones regarding both accuracy and efficiency. Most approaches employ a bag-of-symbolic-words representation, but typically the word-length is fixed across all time series, an issue identified as a major weakness in the literature. Also, there are no prior attempts to use efficient sequence learning techniques to go beyond single words, to features based on variable-length sequences of words or symbols. We study an efficient linear classification approach, SEQL, originally designed for classification of symbolic sequences. SEQL learns discriminative subsequences from training data by exploiting the all-subsequence space using greedy gradient descent. We explore different discretization approaches, from none at all to increasing smoothing of the original data, and study the effect of these transformations on the accuracy of SEQL classifiers. We propose two adaptations of SEQL for time series data, SAX-VSEQL, can deal with X-axis offsets by learning variable-length symbolic words, and SAX-VFSEQL, can deal with X-axis and Y-axis offsets, by learning fuzzy variable-length symbolic words. Our models are linear classifiers in rich feature spaces. Their predictions are based on the most discriminative subsequences learned during training, and can be investigated for interpreting the classification decision.", "color": {"background": "#aec7e8", "border": "#d62728", "highlight": {"background": "#aec7e8", "border": "#d62728"}, "hover": {"background": "#aec7e8", "border": "#d62728"}}, "id": "c293e24548c22efbb1b856d7c4f53242300df88e", "label": "c293e24548c22efbb1b856d7c4f53242300df88e", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e c293e24548c22efbb1b856d7c4f53242300df88e\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2017.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Time Series Classification by Sequence Learning in All-Subsequence Space\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eExisting approaches to time series classification can be grouped into shape-based (numeric) and structure-based (symbolic). Shape-based techniques use the raw numeric time series with Euclidean or Dynamic Time Warping distance and a 1-Nearest Neighbor classifier. They are accurate, but computationally intensive. Structure-based methods discretize the raw data into symbolic representations, then extract features for classifiers. Recent symbolic methods have outperformed numeric ones regarding both accuracy and efficiency. Most approaches employ a bag-of-symbolic-words representation, but typically the word-length is fixed across all time series, an issue identified as a major weakness in the literature. Also, there are no prior attempts to use efficient sequence learning techniques to go beyond single words, to features based on variable-length sequences of words or symbols. We study an efficient linear classification approach, SEQL, originally designed for classification of symbolic sequences. SEQL learns discriminative subsequences from training data by exploiting the all-subsequence space using greedy gradient descent. We explore different discretization approaches, from none at all to increasing smoothing of the original data, and study the effect of these transformations on the accuracy of SEQL classifiers. We propose two adaptations of SEQL for time series data, SAX-VSEQL, can deal with X-axis offsets by learning variable-length symbolic words, and SAX-VFSEQL, can deal with X-axis and Y-axis offsets, by learning fuzzy variable-length symbolic words. Our models are linear classifiers in rich feature spaces. Their predictions are based on the most discriminative subsequences learned during training, and can be investigated for interpreting the classification decision.\u003c/div\u003e"}, {"abstract": "Multivariate time series classi\ufb01cation (MTSC), one of the most fundamental time series applications, has not only gained substantial research attentions but has also emerged in many real-life applications. Recently, using transformers to solve MTSC has been reported. However, current transformer-based methods take data points of individual timestamps as inputs (timestamp-level), which only capture the temporal dependencies, not the dependencies among variables. In this\npaper, we propose a novel method, called SVP-T. Specifically, we \ufb01rst propose to take time series subsequences, which can be from different variables and positions (time interval), as the inputs (shape-level). The temporal and variable dependencies are both handled by capturing the long- and short-term dependencies among shapes. Second, we propose a variable-position encoding layer (VP-layer) to utilize both the variable and position information of each shape. Third, we introduce a novel VP-based (Variable-Position) self-attention mechanism to allow the enhancing the attention weights of overlapping shapes. We evaluate our method on all UEA MTS datasets. SVP-T achieves the best accuracy rank when compared with several competitive state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the VP-layer and the VP-based self-attention mechanism. Finally, we present one case study to interpret the result of SVP-T.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "86f260abb52cea53b4dbf3f5c2a5669450983374", "label": "86f260abb52cea53b4dbf3f5c2a5669450983374", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 86f260abb52cea53b4dbf3f5c2a5669450983374\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e SVP-T: A Shape-Level Variable-Position Transformer for Multivariate Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMultivariate time series classi\ufb01cation (MTSC), one of the most fundamental time series applications, has not only gained substantial research attentions but has also emerged in many real-life applications. Recently, using transformers to solve MTSC has been reported. However, current transformer-based methods take data points of individual timestamps as inputs (timestamp-level), which only capture the temporal dependencies, not the dependencies among variables. In this\npaper, we propose a novel method, called SVP-T. Specifically, we \ufb01rst propose to take time series subsequences, which can be from different variables and positions (time interval), as the inputs (shape-level). The temporal and variable dependencies are both handled by capturing the long- and short-term dependencies among shapes. Second, we propose a variable-position encoding layer (VP-layer) to utilize both the variable and position information of each shape. Third, we introduce a novel VP-based (Variable-Position) self-attention mechanism to allow the enhancing the attention weights of overlapping shapes. We evaluate our method on all UEA MTS datasets. SVP-T achieves the best accuracy rank when compared with several competitive state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the VP-layer and the VP-based self-attention mechanism. Finally, we present one case study to interpret the result of SVP-T.\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "label": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e c0c807b59e6497fe07de537d9eb11fdbd442ecf6\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2013.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "aeon is a unified Python 3 library for all machine learning tasks involving time series. The package contains modules for time series forecasting, classification, extrinsic regression and clustering, as well as a variety of utilities, transformations and distance measures designed for time series data. aeon also has a number of experimental modules for tasks such as anomaly detection, similarity search and segmentation. aeon follows the scikit-learn API as much as possible to help new users and enable easy integration of aeon estimators with useful tools such as model selection and pipelines. It provides a broad library of time series algorithms, including efficient implementations of the very latest advances in research. Using a system of optional dependencies, aeon integrates a wide variety of packages into a single interface while keeping the core framework with minimal dependencies. The package is distributed under the 3-Clause BSD license and is available at https://github.com/ aeon-toolkit/aeon. This version was submitted to the JMLR journal on 02 Nov 2023 for v0.5.0 of aeon. At the time of this preprint aeon has released v0.9.0, and has had substantial changes.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "6664110bc4fee7c79dd1928d2a37dd9222295260", "label": "6664110bc4fee7c79dd1928d2a37dd9222295260", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 6664110bc4fee7c79dd1928d2a37dd9222295260\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2024.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e aeon: a Python toolkit for learning from time series\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eaeon is a unified Python 3 library for all machine learning tasks involving time series. The package contains modules for time series forecasting, classification, extrinsic regression and clustering, as well as a variety of utilities, transformations and distance measures designed for time series data. aeon also has a number of experimental modules for tasks such as anomaly detection, similarity search and segmentation. aeon follows the scikit-learn API as much as possible to help new users and enable easy integration of aeon estimators with useful tools such as model selection and pipelines. It provides a broad library of time series algorithms, including efficient implementations of the very latest advances in research. Using a system of optional dependencies, aeon integrates a wide variety of packages into a single interface while keeping the core framework with minimal dependencies. The package is distributed under the 3-Clause BSD license and is available at https://github.com/ aeon-toolkit/aeon. This version was submitted to the JMLR journal on 02 Nov 2023 for v0.5.0 of aeon. At the time of this preprint aeon has released v0.9.0, and has had substantial changes.\u003c/div\u003e"}, {"abstract": "", "color": "#ff7f0e", "id": "7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb", "label": "7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e End-to-end deep representation learning for time series clustering: a comparative study\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "To improve the accuracy of clinical multivariate time series (MTS) classification (such as EEG and ECG) by a novel self-supervised paradigm that directly captures the dynamics between the different time series learned together to optimize the classification task. Labels in clinical datasets are very often insufficient. One way to address this challenge is leveraging self-supervision. This paradigm attempts to identify a supervisory signal inherent within a dataset to serve as a surrogate label. We present a novel form of self-supervision: dynamics of clinical MTS. Unlike other self-supervision methods, such as masking, that are intuitive but still heuristic, we suggest to learn a representation justified by Koopman theory. The latter was shown useful for representing clinical time series and can be used as a form of surrogate task to improve the clinical MTS classification. In the ECG task, we show that our proposed framework achieved higher sensitivity and specificity than the state-of-the-art (SOTA) baseline over numerous common diagnoses. For EEG abnormality classification, our proposed framework also achieved higher sensitivity and specificity than the SOTA baseline. All results are statistically significant. Our technique yields reliable clinical diagnosis in an empirical study employing signals from thousands of patients in multiple clinical tasks employing two types of clinical-grade sensors (ECG and EEG) as compared to the state-of-the-art machine learning. Leveraging time-series-dynamics self-supervision can help mitigate the lack of labels in clinical datasets used for training machine learning algorithms and significantly improve their performance. Specifically, the ECG system presented in this work is being trialed in hospitals, used by top cardiologists for patient diagnosis and treatment. We believe that the deployment of such cutting-edge technology will significantly improve the accuracy and speed of cardiac assessments.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "de72c6226ecbc7150dc3e634ca983d001808e6b0", "label": "de72c6226ecbc7150dc3e634ca983d001808e6b0", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e de72c6226ecbc7150dc3e634ca983d001808e6b0\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Self-supervised Classification of Clinical Multivariate Time Series using Time Series Dynamics\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eTo improve the accuracy of clinical multivariate time series (MTS) classification (such as EEG and ECG) by a novel self-supervised paradigm that directly captures the dynamics between the different time series learned together to optimize the classification task. Labels in clinical datasets are very often insufficient. One way to address this challenge is leveraging self-supervision. This paradigm attempts to identify a supervisory signal inherent within a dataset to serve as a surrogate label. We present a novel form of self-supervision: dynamics of clinical MTS. Unlike other self-supervision methods, such as masking, that are intuitive but still heuristic, we suggest to learn a representation justified by Koopman theory. The latter was shown useful for representing clinical time series and can be used as a form of surrogate task to improve the clinical MTS classification. In the ECG task, we show that our proposed framework achieved higher sensitivity and specificity than the state-of-the-art (SOTA) baseline over numerous common diagnoses. For EEG abnormality classification, our proposed framework also achieved higher sensitivity and specificity than the SOTA baseline. All results are statistically significant. Our technique yields reliable clinical diagnosis in an empirical study employing signals from thousands of patients in multiple clinical tasks employing two types of clinical-grade sensors (ECG and EEG) as compared to the state-of-the-art machine learning. Leveraging time-series-dynamics self-supervision can help mitigate the lack of labels in clinical datasets used for training machine learning algorithms and significantly improve their performance. Specifically, the ECG system presented in this work is being trialed in hospitals, used by top cardiologists for patient diagnosis and treatment. We believe that the deployment of such cutting-edge technology will significantly improve the accuracy and speed of cardiac assessments.\u003c/div\u003e"}, {"abstract": "In 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website this http URL. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.", "color": {"background": "#aec7e8", "border": "#d62728", "highlight": {"background": "#aec7e8", "border": "#d62728"}, "hover": {"background": "#aec7e8", "border": "#d62728"}}, "id": "d8abb8206b913d185b4bd406880131c13759a6ff", "label": "d8abb8206b913d185b4bd406880131c13759a6ff", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e d8abb8206b913d185b4bd406880131c13759a6ff\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2018.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e The UEA multivariate time series classification archive, 2018\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eIn 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website this http URL. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.\u003c/div\u003e"}, {"abstract": "In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future.", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "label": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e a17fa87dc3eeebff1da725f60ef8a1608eb7986d\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2016.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eIn the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future.\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "6dc740628f9d95bc3d6396f4b3a16b57b9bca4af", "label": "6dc740628f9d95bc3d6396f4b3a16b57b9bca4af", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 6dc740628f9d95bc3d6396f4b3a16b57b9bca4af\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2015.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Binary Shapelet Transform for Multiclass Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "This paper contributes multivariate versions of seven commonly used elastic similarity and distance measures for time series data analytics. Elastic similarity and distance measures can compensate for misalignments in the time axis of time series data. We adapt two existing strategies used in a multivariate version of the well-known Dynamic Time Warping (DTW), namely, Independent and Dependent DTW, to these seven measures. While these measures can be applied to various time series analysis tasks, we demonstrate their utility on multivariate time series classification using the nearest neighbor classifier. On 23 well-known datasets, we demonstrate that each of the measures but one achieves the highest accuracy relative to others on at least one dataset, supporting the value of developing a suite of multivariate similarity and distance measures. We also demonstrate that there are datasets for which either the dependent versions of all measures are more accurate than their independent counterparts or vice versa. In addition, we also construct a nearest neighbor-based ensemble of the measures and show that it is competitive to other state-of-the-art single-strategy multivariate time series classifiers.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "label": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 89c0f84361a34c6224c3a1a14ead80ed3c82c9f0\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Elastic similarity and distance measures for multivariate time series\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThis paper contributes multivariate versions of seven commonly used elastic similarity and distance measures for time series data analytics. Elastic similarity and distance measures can compensate for misalignments in the time axis of time series data. We adapt two existing strategies used in a multivariate version of the well-known Dynamic Time Warping (DTW), namely, Independent and Dependent DTW, to these seven measures. While these measures can be applied to various time series analysis tasks, we demonstrate their utility on multivariate time series classification using the nearest neighbor classifier. On 23 well-known datasets, we demonstrate that each of the measures but one achieves the highest accuracy relative to others on at least one dataset, supporting the value of developing a suite of multivariate similarity and distance measures. We also demonstrate that there are datasets for which either the dependent versions of all measures are more accurate than their independent counterparts or vice versa. In addition, we also construct a nearest neighbor-based ensemble of the measures and show that it is competitive to other state-of-the-art single-strategy multivariate time series classifiers.\u003c/div\u003e"}, {"abstract": "This article proposes an efficient federated distillation learning system (EFDLS) for multitask time series classification (TSC). EFDLS consists of a central server and multiple mobile users, where different users may run different TSC tasks. EFDLS has two novel components: a feature-based student\u2013teacher (FBST) framework and a distance-based weights matching (DBWM) scheme. For each user, the FBST framework transfers knowledge from its teacher\u2019s hidden layers to its student\u2019s hidden layers via knowledge distillation, where the teacher and student have identical network structures. For each connected user, its student model\u2019s hidden layers\u2019 weights are uploaded to the EFDLS server periodically. The DBWM scheme is deployed on the server, with the least square distance (LSD) used to measure the similarity between the weights of two given models. This scheme finds a partner for each connected user such that the user\u2019s and its partner\u2019s weights are the closest among all the weights uploaded. The server exchanges and sends back the user\u2019s and its partner\u2019s weights to these two users which then load the received weights to their teachers\u2019 hidden layers. Experimental results show that compared with a number of state-of-the-art federated learning (FL) algorithms, our proposed EFDLS wins 20 out of 44 standard UCR2018 datasets and achieves the highest mean accuracy (70.14%) on these datasets. In particular, compared with a single-task baseline, EFDLS obtains 32/4/8 regarding \u201cwin\u201d/\u201ctie\u201d/\u201close\u201d and results in an improvement of approximately 4% in terms of mean accuracy.", "color": "#ff7f0e", "id": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "label": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 8991f2c72b0a8d5af83f1f3c03e49514d0b20eee\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e An Efficient Federated Distillation Learning System for Multitask Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThis article proposes an efficient federated distillation learning system (EFDLS) for multitask time series classification (TSC). EFDLS consists of a central server and multiple mobile users, where different users may run different TSC tasks. EFDLS has two novel components: a feature-based student\u2013teacher (FBST) framework and a distance-based weights matching (DBWM) scheme. For each user, the FBST framework transfers knowledge from its teacher\u2019s hidden layers to its student\u2019s hidden layers via knowledge distillation, where the teacher and student have identical network structures. For each connected user, its student model\u2019s hidden layers\u2019 weights are uploaded to the EFDLS server periodically. The DBWM scheme is deployed on the server, with the least square distance (LSD) used to measure the similarity between the weights of two given models. This scheme finds a partner for each connected user such that the user\u2019s and its partner\u2019s weights are the closest among all the weights uploaded. The server exchanges and sends back the user\u2019s and its partner\u2019s weights to these two users which then load the received weights to their teachers\u2019 hidden layers. Experimental results show that compared with a number of state-of-the-art federated learning (FL) algorithms, our proposed EFDLS wins 20 out of 44 standard UCR2018 datasets and achieves the highest mean accuracy (70.14%) on these datasets. In particular, compared with a single-task baseline, EFDLS obtains 32/4/8 regarding \u201cwin\u201d/\u201ctie\u201d/\u201close\u201d and results in an improvement of approximately 4% in terms of mean accuracy.\u003c/div\u003e"}, {"abstract": "Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer.The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "label": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 5b7f5488c380cf5085a5dd93e993ad293b225eee\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e One Fits All: Power General Time Series Analysis by Pretrained LM\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eAlthough we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer.The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.\u003c/div\u003e"}, {"abstract": "The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. The research focus has mostly been on improving the accuracy and efficiency of classifiers, while their interpretability has been somewhat neglected. Classifier interpretability has become a critical constraint for many application domains and the introduction of the \u0026#x27;right to explanation\u0026#x27; GDPR EU legislation in May 2018 is likely to further emphasize the importance of explainable learning algorithms. In this work we analyse the state-of-the-art for time series classification, and propose new algorithms that aim to maintain the classifier accuracy and efficiency, but keep interpretability as a key design constraint. We present new time series classification algorithms that advance the state-of-the-art by implementing the following three key ideas: (1) Multiple resolutions of symbolic approximations: we combine symbolic representations obtained using different parameters; (2) Multiple domain representations: we combine symbolic approximations in time (e.g., SAX) and frequency (e.g., SFA) domains; (3) Efficient navigation of a huge symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL), and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that a multi-resolution multi-domain linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but uses a fraction of the time required by either COTE or FCN. We discuss the accuracy, efficiency and interpretability of our proposed algorithms. To further analyse the interpretability aspect of our classifiers, we present a case study on an ecology benchmark.", "color": "#aec7e8", "id": "3665bec103533566d7580441227f35a000c9b5ff", "label": "3665bec103533566d7580441227f35a000c9b5ff", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 3665bec103533566d7580441227f35a000c9b5ff\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2018.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Interpretable Time Series Classification using All-Subsequence Learning and Symbolic Representations in Time and Frequency Domains\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThe time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. The research focus has mostly been on improving the accuracy and efficiency of classifiers, while their interpretability has been somewhat neglected. Classifier interpretability has become a critical constraint for many application domains and the introduction of the \u0026#x27;right to explanation\u0026#x27; GDPR EU legislation in May 2018 is likely to further emphasize the importance of explainable learning algorithms. In this work we analyse the state-of-the-art for time series classification, and propose new algorithms that aim to maintain the classifier accuracy and efficiency, but keep interpretability as a key design constraint. We present new time series classification algorithms that advance the state-of-the-art by implementing the following three key ideas: (1) Multiple resolutions of symbolic approximations: we combine symbolic representations obtained using different parameters; (2) Multiple domain representations: we combine symbolic approximations in time (e.g., SAX) and frequency (e.g., SFA) domains; (3) Efficient navigation of a huge symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL), and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that a multi-resolution multi-domain linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but uses a fraction of the time required by either COTE or FCN. We discuss the accuracy, efficiency and interpretability of our proposed algorithms. To further analyse the interpretability aspect of our classifiers, we present a case study on an ecology benchmark.\u003c/div\u003e"}, {"abstract": "The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. Prior research has mostly focused on improving the accuracy and efficiency of classifiers, with interpretability being somewhat neglected. This aspect of classifiers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classification accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not efficient with regard to either time or space, are difficult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set fixed-length. In this paper we propose new time series classification algorithms to address these gaps. Our approach is based on symbolic representations of time series, efficient sequence mining algorithms and linear classification models. Our linear models are as accurate as deep learning models but are more efficient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We advance the state-of-the-art in time series classification by proposing new algorithms built using the following three key ideas: (1) Multiple resolutions of symbolic representations: we combine symbolic representations obtained using different parameters, rather than one fixed representation (e.g., multiple SAX representations); (2) Multiple domain representations: we combine symbolic representations in time (e.g., SAX) and frequency (e.g., SFA) domains, to be more robust across problem types; (3) Efficient navigation in a huge symbolic-words space: we extend a symbolic sequence classifier (SEQL) to work with multiple symbolic representations and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that our multi-resolution multi-domain linear classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classifier, we present a case study on a human motion dataset collected by the authors. We discuss the accuracy, efficiency and interpretability of our proposed algorithms and release all the results, source code and data to encourage reproducibility.", "color": {"background": "#aec7e8", "border": "#d62728", "highlight": {"background": "#aec7e8", "border": "#d62728"}, "hover": {"background": "#aec7e8", "border": "#d62728"}}, "id": "19c78592bc557b65923ad83e7af26d478721e90f", "label": "19c78592bc557b65923ad83e7af26d478721e90f", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 19c78592bc557b65923ad83e7af26d478721e90f\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2019.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Interpretable time series classification using linear models and multi-resolution multi-domain symbolic representations\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThe time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. Prior research has mostly focused on improving the accuracy and efficiency of classifiers, with interpretability being somewhat neglected. This aspect of classifiers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classification accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not efficient with regard to either time or space, are difficult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set fixed-length. In this paper we propose new time series classification algorithms to address these gaps. Our approach is based on symbolic representations of time series, efficient sequence mining algorithms and linear classification models. Our linear models are as accurate as deep learning models but are more efficient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We advance the state-of-the-art in time series classification by proposing new algorithms built using the following three key ideas: (1) Multiple resolutions of symbolic representations: we combine symbolic representations obtained using different parameters, rather than one fixed representation (e.g., multiple SAX representations); (2) Multiple domain representations: we combine symbolic representations in time (e.g., SAX) and frequency (e.g., SFA) domains, to be more robust across problem types; (3) Efficient navigation in a huge symbolic-words space: we extend a symbolic sequence classifier (SEQL) to work with multiple symbolic representations and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that our multi-resolution multi-domain linear classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classifier, we present a case study on a human motion dataset collected by the authors. We discuss the accuracy, efficiency and interpretability of our proposed algorithms and release all the results, source code and data to encourage reproducibility.\u003c/div\u003e"}, {"abstract": "Asynchronous Time Series is a multivariate time series where all the channels are observed asynchronously-independently, making the time series extremely sparse when aligning them. We often observe this effect in applications with complex observation processes, such as health care, climate science, and astronomy, to name a few. Because of the asynchronous nature, they pose a significant challenge to deep learning architectures, which presume that the time series presented to them are regularly sampled, fully observed, and aligned with respect to time. This paper proposes a novel framework, that we call Deep Convolutional Set Functions (DCSF), which is highly scalable and memory efficient, for the asynchronous time series classification task. With the recent advancements in deep set learning architectures, we introduce a model that is invariant to the order in which time series\u2019 channels are presented to it. We explore convolutional neural networks, which are well researched for the closely related problem-classification of regularly sampled and fully observed time series, for encoding the set elements. We evaluate DCSF for AsTS classification, and online (per time point) AsTS classification. Our extensive experiments on multiple real world and synthetic datasets verify that the suggested model performs substantially better than a range of state-of-the-art models in terms of accuracy and run time. We increase the accuracy of the mini-Physionet dataset upto 2%; real datasets with synthetic setups of both AsTS, and TSMV upto 30%.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5", "label": "551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e DCSF: Deep Convolutional Set Functions for Classification of Asynchronous Time Series\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eAsynchronous Time Series is a multivariate time series where all the channels are observed asynchronously-independently, making the time series extremely sparse when aligning them. We often observe this effect in applications with complex observation processes, such as health care, climate science, and astronomy, to name a few. Because of the asynchronous nature, they pose a significant challenge to deep learning architectures, which presume that the time series presented to them are regularly sampled, fully observed, and aligned with respect to time. This paper proposes a novel framework, that we call Deep Convolutional Set Functions (DCSF), which is highly scalable and memory efficient, for the asynchronous time series classification task. With the recent advancements in deep set learning architectures, we introduce a model that is invariant to the order in which time series\u2019 channels are presented to it. We explore convolutional neural networks, which are well researched for the closely related problem-classification of regularly sampled and fully observed time series, for encoding the set elements. We evaluate DCSF for AsTS classification, and online (per time point) AsTS classification. Our extensive experiments on multiple real world and synthetic datasets verify that the suggested model performs substantially better than a range of state-of-the-art models in terms of accuracy and run time. We increase the accuracy of the mini-Physionet dataset upto 2%; real datasets with synthetic setups of both AsTS, and TSMV upto 30%.\u003c/div\u003e"}, {"abstract": "Time-series data contains temporal order information that can guide representation learning for predictive end tasks (e.g., classification, regression). Recently, there are some attempts to leverage such order information to first pre-train time-series models by reconstructing time-series values of randomly masked time segments, followed by an end-task fine-tuning on the same dataset, demonstrating improved end-task performance. However, this learning paradigm decouples data reconstruction from the end task. We argue that the representations learnt in this way are not informed by the end task and may, therefore, be sub-optimal for the end-task performance. In fact, the importance of different timestamps can vary significantly in different end tasks. We believe that representations learnt by reconstructing important timestamps would be a better strategy for improving end-task performance. In this work, we propose TARNet, Task-Aware Reconstruction Network, a new model using Transformers to learn task-aware data reconstruction that augments end-task performance. Specifically, we design a data-driven masking strategy that uses self-attention score distribution from end-task training to sample timestamps deemed important by the end task. Then, we mask out data at those timestamps and reconstruct them, thereby making the reconstruction task-aware. This reconstruction task is trained alternately with the end task at every epoch, sharing parameters in a single model, allowing the representation learnt through reconstruction to improve end-task performance. Extensive experiments on tens of classification and regression datasets show that TARNet significantly outperforms state-of-the-art baseline models across all evaluation metrics.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "label": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 3848daf38c39983650a3f6ecb4ccd11dbfab757a\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2022.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e TARNet: Task-Aware Reconstruction for Time-Series Transformer\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eTime-series data contains temporal order information that can guide representation learning for predictive end tasks (e.g., classification, regression). Recently, there are some attempts to leverage such order information to first pre-train time-series models by reconstructing time-series values of randomly masked time segments, followed by an end-task fine-tuning on the same dataset, demonstrating improved end-task performance. However, this learning paradigm decouples data reconstruction from the end task. We argue that the representations learnt in this way are not informed by the end task and may, therefore, be sub-optimal for the end-task performance. In fact, the importance of different timestamps can vary significantly in different end tasks. We believe that representations learnt by reconstructing important timestamps would be a better strategy for improving end-task performance. In this work, we propose TARNet, Task-Aware Reconstruction Network, a new model using Transformers to learn task-aware data reconstruction that augments end-task performance. Specifically, we design a data-driven masking strategy that uses self-attention score distribution from end-task training to sample timestamps deemed important by the end task. Then, we mask out data at those timestamps and reconstruct them, thereby making the reconstruction task-aware. This reconstruction task is trained alternately with the end task at every epoch, sharing parameters in a single model, allowing the representation learnt through reconstruction to improve end-task performance. Extensive experiments on tens of classification and regression datasets show that TARNet significantly outperforms state-of-the-art baseline models across all evaluation metrics.\u003c/div\u003e"}, {"abstract": "", "color": "#2ca02c", "id": "62acca98a5bd6ffb3071e9c2a2f559950e4301fc", "label": "62acca98a5bd6ffb3071e9c2a2f559950e4301fc", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 62acca98a5bd6ffb3071e9c2a2f559950e4301fc\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e nan\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e \u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "Time series shapelets are short discriminative subsequences that recently have been found not only to be accurate but also interpretable for the classification problem of univariate time series (UTS). However, existing work on shapelets selection cannot be applied to multivariate time series classification (MTSC) since the candidate shapelets of MTSC may come from different variables of different lengths and thus cannot be directly compared. To address this challenge, in this paper, we propose a novel model called ShapeNet, which embeds shapelet candidates of different lengths into a unified space for shapelet selection. The network is trained using cluster-wise triplet loss, which considers the distance between anchor and multiple positive (negative) samples and the distance between positive (negative) samples, which are important for convergence. We compute representative and diversified final shapelets rather than directly using all the embeddings for model building to avoid a large fraction of non-discriminative shapelet candidates. We have conducted experiments on ShapeNet with competitive state-of-the-art and benchmark methods using UEA MTS datasets. The results show that the accuracy of ShapeNet is the best of all the methods compared. Furthermore, we illustrate the shapelets\u2019 interpretability with two case studies.", "color": "#ff7f0e", "id": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "label": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e be818bd3db1928dcfe24f23ee7afaa09545f9a61\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eTime series shapelets are short discriminative subsequences that recently have been found not only to be accurate but also interpretable for the classification problem of univariate time series (UTS). However, existing work on shapelets selection cannot be applied to multivariate time series classification (MTSC) since the candidate shapelets of MTSC may come from different variables of different lengths and thus cannot be directly compared. To address this challenge, in this paper, we propose a novel model called ShapeNet, which embeds shapelet candidates of different lengths into a unified space for shapelet selection. The network is trained using cluster-wise triplet loss, which considers the distance between anchor and multiple positive (negative) samples and the distance between positive (negative) samples, which are important for convergence. We compute representative and diversified final shapelets rather than directly using all the embeddings for model building to avoid a large fraction of non-discriminative shapelet candidates. We have conducted experiments on ShapeNet with competitive state-of-the-art and benchmark methods using UEA MTS datasets. The results show that the accuracy of ShapeNet is the best of all the methods compared. Furthermore, we illustrate the shapelets\u2019 interpretability with two case studies.\u003c/div\u003e"}, {"abstract": "Time series classification is an increasing research topic due to the vast amount of time series data that is being created over a wide variety of fields. The particularity of the data makes it a challenging task and different approaches have been taken, including the distance based approach. 1-NN has been a widely used method within distance based time series classification due to its simplicity but still good performance. However, its supremacy may be attributed to being able to use specific distances for time series within the classification process and not to the classifier itself. With the aim of exploiting these distances within more complex classifiers, new approaches have arisen in the past few years that are competitive or which outperform the 1-NN based approaches. In some cases, these new methods use the distance measure to transform the series into feature vectors, bridging the gap between time series and traditional classifiers. In other cases, the distances are employed to obtain a time series kernel and enable the use of kernel methods for time series classification. One of the main challenges is that a kernel function must be positive semi-definite, a matter that is also addressed within this review. The presented review includes a taxonomy of all those methods that aim to classify time series using a distance based approach, as well as a discussion of the strengths and weaknesses of each method.", "color": "#aec7e8", "id": "1a22c4fcff260a8623a1fd677f2e7a7916343dae", "label": "1a22c4fcff260a8623a1fd677f2e7a7916343dae", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 1a22c4fcff260a8623a1fd677f2e7a7916343dae\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2018.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e A review on distance based time series classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eTime series classification is an increasing research topic due to the vast amount of time series data that is being created over a wide variety of fields. The particularity of the data makes it a challenging task and different approaches have been taken, including the distance based approach. 1-NN has been a widely used method within distance based time series classification due to its simplicity but still good performance. However, its supremacy may be attributed to being able to use specific distances for time series within the classification process and not to the classifier itself. With the aim of exploiting these distances within more complex classifiers, new approaches have arisen in the past few years that are competitive or which outperform the 1-NN based approaches. In some cases, these new methods use the distance measure to transform the series into feature vectors, bridging the gap between time series and traditional classifiers. In other cases, the distances are employed to obtain a time series kernel and enable the use of kernel methods for time series classification. One of the main challenges is that a kernel function must be positive semi-definite, a matter that is also addressed within this review. The presented review includes a taxonomy of all those methods that aim to classify time series using a distance based approach, as well as a discussion of the strengths and weaknesses of each method.\u003c/div\u003e"}, {"abstract": "A time series is a sequence of sequentially ordered real values in time. Time series classification (TSC) is the task of assigning a time series to one of a set of predefined classes, usually based on a model learned from examples. Dictionary-based methods for TSC rely on counting the frequency of certain patterns in time series and are important components of the currently most accurate TSC ensembles. One of the early dictionary-based methods was WEASEL, which at its time achieved SotA results while also being very fast. However, it is outperformed both in terms of speed and accuracy by other methods. Furthermore, its design leads to an unpredictably large memory footprint, making it inapplicable for many applications. In this paper, we present WEASEL 2.0, a complete overhaul of WEASEL based on two recent advancements in TSC: Dilation and ensembling of randomized hyper-parameter settings. These two techniques allow WEASEL 2.0 to work with a fixed-size memory footprint while at the same time improving accuracy. Compared to 15 other SotA methods on the UCR benchmark set, WEASEL 2.0 is significantly more accurate than other dictionary methods and not significantly worse than the currently best methods. Actually, it achieves the highest median accuracy over all data sets, and it performs best in 5 out of 12 problem classes. We thus believe that WEASEL 2.0 is a viable alternative for current TSC and also a potentially interesting input for future ensembles.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "4add06e705ea1e6d3179eca1fde473441916c6b2", "label": "4add06e705ea1e6d3179eca1fde473441916c6b2", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 4add06e705ea1e6d3179eca1fde473441916c6b2\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e WEASEL 2.0 - A Random Dilated Dictionary Transform for Fast, Accurate and Memory Constrained Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eA time series is a sequence of sequentially ordered real values in time. Time series classification (TSC) is the task of assigning a time series to one of a set of predefined classes, usually based on a model learned from examples. Dictionary-based methods for TSC rely on counting the frequency of certain patterns in time series and are important components of the currently most accurate TSC ensembles. One of the early dictionary-based methods was WEASEL, which at its time achieved SotA results while also being very fast. However, it is outperformed both in terms of speed and accuracy by other methods. Furthermore, its design leads to an unpredictably large memory footprint, making it inapplicable for many applications. In this paper, we present WEASEL 2.0, a complete overhaul of WEASEL based on two recent advancements in TSC: Dilation and ensembling of randomized hyper-parameter settings. These two techniques allow WEASEL 2.0 to work with a fixed-size memory footprint while at the same time improving accuracy. Compared to 15 other SotA methods on the UCR benchmark set, WEASEL 2.0 is significantly more accurate than other dictionary methods and not significantly worse than the currently best methods. Actually, it achieves the highest median accuracy over all data sets, and it performs best in 5 out of 12 problem classes. We thus believe that WEASEL 2.0 is a viable alternative for current TSC and also a potentially interesting input for future ensembles.\u003c/div\u003e"}, {"abstract": "Abstract Time series are a critical component of ecological analysis, used to track changes in biotic and abiotic variables. Information can be extracted from the properties of time series for tasks such as classification (e.g., assigning species to individual bird calls); clustering (e.g., clustering similar responses in population dynamics to abrupt changes in the environment or management interventions); prediction (e.g., accuracy of model predictions to original time series data); and anomaly detection (e.g., detecting possible catastrophic events from population time series). These common tasks in ecological research all rely on the notion of (dis\u2010) similarity, which can be determined using distance measures. A plethora of distance measures have been described, predominantly in the computer and information sciences, but many have not been introduced to ecologists. Furthermore, little is known about how to select appropriate distance measures for time\u2010series\u2010related tasks. Therefore, many potential applications remain unexplored. Here, we describe 16 properties of distance measures that are likely to be of importance to a variety of ecological questions involving time series. We then test 42 distance measures for each property and use the results to develop an objective method to select appropriate distance measures for any task and ecological dataset. We demonstrate our selection method by applying it to a set of real\u2010world data on breeding bird populations in the UK and discuss other potential applications for distance measures, along with associated technical issues common in ecology. Our real\u2010world population trends exhibit a common challenge for time series comparisons: a high level of stochasticity. We demonstrate two different ways of overcoming this challenge, first by selecting distance measures with properties that make them well suited to comparing noisy time series and second by applying a smoothing algorithm before selecting appropriate distance measures. In both cases, the distance measures chosen through our selection method are not only fit\u2010for\u2010purpose but are consistent in their rankings of the population trends. The results of our study should lead to an improved understanding of, and greater scope for, the use of distance measures for comparing ecological time series and help us answer new ecological questions.", "color": "#2ca02c", "id": "45e795ef4736bf2a90b9037d22c40320d478953c", "label": "45e795ef4736bf2a90b9037d22c40320d478953c", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 45e795ef4736bf2a90b9037d22c40320d478953c\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e A user\u2010friendly guide to using distance measures to compare time series in ecology\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eAbstract Time series are a critical component of ecological analysis, used to track changes in biotic and abiotic variables. Information can be extracted from the properties of time series for tasks such as classification (e.g., assigning species to individual bird calls); clustering (e.g., clustering similar responses in population dynamics to abrupt changes in the environment or management interventions); prediction (e.g., accuracy of model predictions to original time series data); and anomaly detection (e.g., detecting possible catastrophic events from population time series). These common tasks in ecological research all rely on the notion of (dis\u2010) similarity, which can be determined using distance measures. A plethora of distance measures have been described, predominantly in the computer and information sciences, but many have not been introduced to ecologists. Furthermore, little is known about how to select appropriate distance measures for time\u2010series\u2010related tasks. Therefore, many potential applications remain unexplored. Here, we describe 16 properties of distance measures that are likely to be of importance to a variety of ecological questions involving time series. We then test 42 distance measures for each property and use the results to develop an objective method to select appropriate distance measures for any task and ecological dataset. We demonstrate our selection method by applying it to a set of real\u2010world data on breeding bird populations in the UK and discuss other potential applications for distance measures, along with associated technical issues common in ecology. Our real\u2010world population trends exhibit a common challenge for time series comparisons: a high level of stochasticity. We demonstrate two different ways of overcoming this challenge, first by selecting distance measures with properties that make them well suited to comparing noisy time series and second by applying a smoothing algorithm before selecting appropriate distance measures. In both cases, the distance measures chosen through our selection method are not only fit\u2010for\u2010purpose but are consistent in their rankings of the population trends. The results of our study should lead to an improved understanding of, and greater scope for, the use of distance measures for comparing ecological time series and help us answer new ecological questions.\u003c/div\u003e"}, {"abstract": "The SIFT framework has shown to be effective in the image classification context. In [4], we designed a Bag-of-Words approach based on an adaptation of this framework to time series classification. It relies on two steps: SIFT-based features are first extracted and quantized into words; histograms of occurrences of each word are then fed into a classifier. In this paper, we investigate techniques to improve the performance of Bag-of-Temporal-SIFT-Words: dense extraction of keypoints and different normalizations of Bag-of-Words histograms. Extensive experiments show that our method significantly outperforms nearly all tested standalone baseline classifiers on publicly available UCR datasets.", "color": "#1f77b4", "id": "8dfaf780e78545388e527f66b2e58366599c6cd1", "label": "8dfaf780e78545388e527f66b2e58366599c6cd1", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 8dfaf780e78545388e527f66b2e58366599c6cd1\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2016.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e Dense Bag-of-Temporal-SIFT-Words for Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eThe SIFT framework has shown to be effective in the image classification context. In [4], we designed a Bag-of-Words approach based on an adaptation of this framework to time series classification. It relies on two steps: SIFT-based features are first extracted and quantized into words; histograms of occurrences of each word are then fed into a classifier. In this paper, we investigate techniques to improve the performance of Bag-of-Temporal-SIFT-Words: dense extraction of keypoints and different normalizations of Bag-of-Words histograms. Extensive experiments show that our method significantly outperforms nearly all tested standalone baseline classifiers on publicly available UCR datasets.\u003c/div\u003e"}, {"abstract": "Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "label": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e d61f58df7495c7243252dd1aa43dbf8bf58c9500\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2020.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eTime Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.\u003c/div\u003e"}, {"abstract": "Symbolic representations of time series have proven to be effective for time series classification, with many recent approaches including BOSS, WEASEL, and MrSEQL. The key idea is to transform numerical time series to symbolic representations in the time or frequency domain, i.e., sequences of symbols, and then extract features from these sequences. While achieving high accuracy, existing symbolic classifiers are computationally expensive. It is also not clear whether further accuracy and speed improvements could be gained by a careful analysis of the symbolic transform and the trade-offs between time domain and frequency domain symbolic features. In this paper we present MrSQM, a new time series classifier that uses multiple symbolic representations and efficient sequence mining, to extract important time series features. We study two symbolic transforms and four feature selection approaches on symbolic sequences, ranging from fully supervised, to unsupervised and hybrids. We propose a new approach for optimal supervised symbolic feature selection in all-subsequence space, by adapting a Chi-squared bound developed for discriminative pattern mining, to time series. Our experiments on the 112 datasets of the UEA/UCR benchmark demonstrate that MrSQM can quickly extract useful features and learn accurate classifiers with the logistic regression algorithm. We show that a fast symbolic transform combined with a simple feature selection strategy can be highly effective as compared to more sophisticated and expensive feature selection methods. MrSQM completes training and prediction on 112 UEA/UCR datasets in 1.5h for an accuracy comparable to existing efficient state-of-the-art methods, e.g., MrSEQL (10h) and ROCKET (2.5h). Furthermore, MrSQM enables the user to trade-off accuracy and speed by controlling the type and number of symbolic representations, thus further reducing the total runtime to 20 minutes for a similar level of accuracy. Thach Le Nguyen and Georgiana Ifrim are with the School of Computer Science, Insight Centre for Data Analytics, University College Dublin, Ireland. {thach.lenguyen, georgiana.ifrim}@ucd.ie ar X iv :2 10 9. 01 03 6v 2 [ cs .L G ] 1 4 M ar 2 02 2 2 Thach Le Nguyen, Georgiana Ifrim", "color": {"background": "#ff7f0e", "border": "#d62728", "highlight": {"background": "#ff7f0e", "border": "#d62728"}, "hover": {"background": "#ff7f0e", "border": "#d62728"}}, "id": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "label": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 57e1a9508f0e0d428b4dfd49d911fae14494778e\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2021.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e MrSQM: Fast Time Series Classification with Symbolic Representations\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eSymbolic representations of time series have proven to be effective for time series classification, with many recent approaches including BOSS, WEASEL, and MrSEQL. The key idea is to transform numerical time series to symbolic representations in the time or frequency domain, i.e., sequences of symbols, and then extract features from these sequences. While achieving high accuracy, existing symbolic classifiers are computationally expensive. It is also not clear whether further accuracy and speed improvements could be gained by a careful analysis of the symbolic transform and the trade-offs between time domain and frequency domain symbolic features. In this paper we present MrSQM, a new time series classifier that uses multiple symbolic representations and efficient sequence mining, to extract important time series features. We study two symbolic transforms and four feature selection approaches on symbolic sequences, ranging from fully supervised, to unsupervised and hybrids. We propose a new approach for optimal supervised symbolic feature selection in all-subsequence space, by adapting a Chi-squared bound developed for discriminative pattern mining, to time series. Our experiments on the 112 datasets of the UEA/UCR benchmark demonstrate that MrSQM can quickly extract useful features and learn accurate classifiers with the logistic regression algorithm. We show that a fast symbolic transform combined with a simple feature selection strategy can be highly effective as compared to more sophisticated and expensive feature selection methods. MrSQM completes training and prediction on 112 UEA/UCR datasets in 1.5h for an accuracy comparable to existing efficient state-of-the-art methods, e.g., MrSEQL (10h) and ROCKET (2.5h). Furthermore, MrSQM enables the user to trade-off accuracy and speed by controlling the type and number of symbolic representations, thus further reducing the total runtime to 20 minutes for a similar level of accuracy. Thach Le Nguyen and Georgiana Ifrim are with the School of Computer Science, Insight Centre for Data Analytics, University College Dublin, Ireland. {thach.lenguyen, georgiana.ifrim}@ucd.ie ar X iv :2 10 9. 01 03 6v 2 [ cs .L G ] 1 4 M ar 2 02 2 2 Thach Le Nguyen, Georgiana Ifrim\u003c/div\u003e"}, {"abstract": "", "color": {"background": "#1f77b4", "border": "#d62728", "highlight": {"background": "#1f77b4", "border": "#d62728"}, "hover": {"background": "#1f77b4", "border": "#d62728"}}, "id": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "label": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 44202c42e2387c61acd1261d7a750d3f520ca0c7\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2016.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u003c/div\u003e"}, {"abstract": "Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class. This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details. In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features. In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set. We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series. We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others. In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes. For each module, we employ the transformer encoder to capture the correlation between their features. As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance. Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods. The code is available at https://github.com/xuanmay2701/shapeformer.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "68106f815739c305a6375c9d593a1aac586ee9ed", "label": "68106f815739c305a6375c9d593a1aac586ee9ed", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 68106f815739c305a6375c9d593a1aac586ee9ed\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2024.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMultivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class. This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details. In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features. In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set. We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series. We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others. In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes. For each module, we employ the transformer encoder to capture the correlation between their features. As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance. Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods. The code is available at https://github.com/xuanmay2701/shapeformer.\u003c/div\u003e"}, {"abstract": "Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets. Additionally, many existing methods focus on a single type of feature such as shape or frequency. Building on the recent success of convolutional neural networks for time series classification, we show that simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods. Using this method, it is possible to train and test a classifier on all 85 \u2018bake off\u2019 datasets in the UCR archive in \u0026lt;2h\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\u0026lt;\\,2\\,\\hbox {h}$$\\end{document}, and it is possible to train a classifier on a large dataset of more than one million time series in approximately 1 h.", "color": {"background": "#aec7e8", "border": "#d62728", "highlight": {"background": "#aec7e8", "border": "#d62728"}, "hover": {"background": "#aec7e8", "border": "#d62728"}}, "id": "2deafb11372f085d504db87fd626e478d8e965aa", "label": "2deafb11372f085d504db87fd626e478d8e965aa", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e 2deafb11372f085d504db87fd626e478d8e965aa\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2019.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMost methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets. Additionally, many existing methods focus on a single type of feature such as shape or frequency. Building on the recent success of convolutional neural networks for time series classification, we show that simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods. Using this method, it is possible to train and test a classifier on all 85 \u2018bake off\u2019 datasets in the UCR archive in \u0026lt;2h\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\u0026lt;\\,2\\,\\hbox {h}$$\\end{document}, and it is possible to train a classifier on a large dataset of more than one million time series in approximately 1 h.\u003c/div\u003e"}, {"abstract": "\u2014Despite the impressive achievements of pre-trained models in the fields of natural language processing (NLP) and computer vision (CV), progress in the domain of time series analysis has been limited. In contrast to NLP and CV, where a single model can handle various tasks, time series analysis still relies heavily on task-specific methods for activities such as classification, anomaly detection, forecasting, and few-shot learning. The primary obstacle to developing a pre-trained model for time series analysis is the scarcity of sufficient training data. In our research, we overcome this obstacle by utilizing pre-trained models from language or CV, which have been trained on billions of data points, and apply them to time series analysis. We assess the effectiveness of the pre-trained transformer model in two ways. Initially, we maintain the original structure of the self-attention and feedforward layers in the residual blocks of the pre-trained language or image model, using the Frozen Pre-trained Transformer (FPT) for time series analysis with the addition of projection matrices for input and output. Additionally, we introduce four unique adapters, designed specifically for downstream tasks based on the pre-trained model, including forecasting and anomaly detection. These adapters are further enhanced with efficient parameter tuning, resulting in superior performance compared to all state-of-the-art methods. Our comprehensive experimental studies reveal that (a) the simple FPT achieves top-tier performance across various time series analysis tasks; and (b) fine-tuning the FPT with the custom-designed adapters can further elevate its performance, outshining specialized task-specific models. As presented in Figure 1, pre-trained models from natural language domains demonstrate remarkable performance, outstripping competitors in all key time series analysis tasks. Furthermore, both theoretical and empirical evidence suggests that the self-attention module behaves analogously to principle component analysis (PCA). This insight is instrumental in understanding how the transformer bridges the domain gap and is a vital step toward grasping the universality of a pre-trained transformer. The code is publicly available at https://", "color": "#2ca02c", "id": "15bfe54223d28c74f9e39960f7ff1d03167e285f", "label": "15bfe54223d28c74f9e39960f7ff1d03167e285f", "shape": "dot", "size": 15, "title": "\u003cb\u003eID:\u003c/b\u003e 15bfe54223d28c74f9e39960f7ff1d03167e285f\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e One Fits All: Universal Time Series Analysis by Pretrained LM and Specially Designed Adaptors\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003e\u2014Despite the impressive achievements of pre-trained models in the fields of natural language processing (NLP) and computer vision (CV), progress in the domain of time series analysis has been limited. In contrast to NLP and CV, where a single model can handle various tasks, time series analysis still relies heavily on task-specific methods for activities such as classification, anomaly detection, forecasting, and few-shot learning. The primary obstacle to developing a pre-trained model for time series analysis is the scarcity of sufficient training data. In our research, we overcome this obstacle by utilizing pre-trained models from language or CV, which have been trained on billions of data points, and apply them to time series analysis. We assess the effectiveness of the pre-trained transformer model in two ways. Initially, we maintain the original structure of the self-attention and feedforward layers in the residual blocks of the pre-trained language or image model, using the Frozen Pre-trained Transformer (FPT) for time series analysis with the addition of projection matrices for input and output. Additionally, we introduce four unique adapters, designed specifically for downstream tasks based on the pre-trained model, including forecasting and anomaly detection. These adapters are further enhanced with efficient parameter tuning, resulting in superior performance compared to all state-of-the-art methods. Our comprehensive experimental studies reveal that (a) the simple FPT achieves top-tier performance across various time series analysis tasks; and (b) fine-tuning the FPT with the custom-designed adapters can further elevate its performance, outshining specialized task-specific models. As presented in Figure 1, pre-trained models from natural language domains demonstrate remarkable performance, outstripping competitors in all key time series analysis tasks. Furthermore, both theoretical and empirical evidence suggests that the self-attention module behaves analogously to principle component analysis (PCA). This insight is instrumental in understanding how the transformer bridges the domain gap and is a vital step toward grasping the universality of a pre-trained transformer. The code is publicly available at https://\u003c/div\u003e"}, {"abstract": "Multivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. To address this, we propose T-Rep, a self-supervised method to learn time series representations at a timestep granularity. T-Rep learns vector embeddings of time alongside its feature extractor, to extract temporal features such as trend, periodicity, or distribution shifts from the signal. These time-embeddings are leveraged in pretext tasks, to incorporate smooth and fine-grained temporal dependencies in the representations, as well as reinforce robustness to missing data. We evaluate T-Rep on downstream classification, forecasting, and anomaly detection tasks. It is compared to existing self-supervised algorithms for time series, which it outperforms in all three tasks. We test T-Rep in missing data regimes, where it proves more resilient than its counterparts. Finally, we provide latent space visualisation experiments, highlighting the interpretability of the learned representations.", "color": {"background": "#2ca02c", "border": "#d62728", "highlight": {"background": "#2ca02c", "border": "#d62728"}, "hover": {"background": "#2ca02c", "border": "#d62728"}}, "id": "e1a2f67b81515fa06699e84e42be2ff519d37267", "label": "e1a2f67b81515fa06699e84e42be2ff519d37267", "shape": "dot", "size": 30, "title": "\u003cb\u003eID:\u003c/b\u003e e1a2f67b81515fa06699e84e42be2ff519d37267\u003cbr\u003e\u003cb\u003eYear:\u003c/b\u003e 2023.0\u003cbr\u003e\u003cb\u003eTitle:\u003c/b\u003e T-Rep: Representation Learning for Time Series using Time-Embeddings\u003cbr\u003e\u003cdiv style=\u0027max-height:150px; overflow:auto; border:1px solid #ccc; padding:4px; background:#f9f9f9;\u0027\u003e\u003cb\u003eAbstract:\u003c/b\u003e\u003cbr\u003eMultivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. To address this, we propose T-Rep, a self-supervised method to learn time series representations at a timestep granularity. T-Rep learns vector embeddings of time alongside its feature extractor, to extract temporal features such as trend, periodicity, or distribution shifts from the signal. These time-embeddings are leveraged in pretext tasks, to incorporate smooth and fine-grained temporal dependencies in the representations, as well as reinforce robustness to missing data. We evaluate T-Rep on downstream classification, forecasting, and anomaly detection tasks. It is compared to existing self-supervised algorithms for time series, which it outperforms in all three tasks. We test T-Rep in missing data regimes, where it proves more resilient than its counterparts. Finally, we provide latent space visualisation experiments, highlighting the interpretability of the learned representations.\u003c/div\u003e"}]);
                  edges = new vis.DataSet([{"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 1}, {"arrows": "to", "color": "red", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "width": 3}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "1a22c4fcff260a8623a1fd677f2e7a7916343dae", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "19c78592bc557b65923ad83e7af26d478721e90f", "width": 1}, {"arrows": "to", "color": "red", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "width": 3}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 1}, {"arrows": "to", "color": "red", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "width": 3}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "8dfaf780e78545388e527f66b2e58366599c6cd1", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "939043b58d41ca05ca228ae3081c9a540f770a47", "width": 1}, {"arrows": "to", "color": "gray", "from": "0b72c321cd990df3b8960fc68e482d2f9e1834ab", "to": "db210fc464d741f3f627190711ec0150d983f280", "width": 1}, {"arrows": "to", "color": "red", "from": "0326a8edf41d09beb171ca306b1de95d8d85857e", "to": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "width": 3}, {"arrows": "to", "color": "red", "from": "0326a8edf41d09beb171ca306b1de95d8d85857e", "to": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "de72c6226ecbc7150dc3e634ca983d001808e6b0", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "e1a2f67b81515fa06699e84e42be2ff519d37267", "width": 3}, {"arrows": "to", "color": "red", "from": "2051548f7681c96d603de932ee23406c525276f9", "to": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "width": 3}, {"arrows": "to", "color": "gray", "from": "fb25011c564139e37e6b912edfff8d61a04893f4", "to": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "f2260a53350a5166f757979f435a3c477dec6d3e", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 1}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "e1a2f67b81515fa06699e84e42be2ff519d37267", "width": 1}, {"arrows": "to", "color": "red", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "2051548f7681c96d603de932ee23406c525276f9", "width": 3}, {"arrows": "to", "color": "gray", "from": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "to": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "width": 1}, {"arrows": "to", "color": "red", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "width": 3}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "width": 1}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "19c78592bc557b65923ad83e7af26d478721e90f", "width": 1}, {"arrows": "to", "color": "red", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "width": 3}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 1}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "c293e24548c22efbb1b856d7c4f53242300df88e", "width": 1}, {"arrows": "to", "color": "red", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "width": 3}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "3665bec103533566d7580441227f35a000c9b5ff", "width": 1}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "8dfaf780e78545388e527f66b2e58366599c6cd1", "width": 1}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "939043b58d41ca05ca228ae3081c9a540f770a47", "width": 1}, {"arrows": "to", "color": "gray", "from": "be9892e207007d5f4c3e23133ea03daef64b87b0", "to": "db210fc464d741f3f627190711ec0150d983f280", "width": 1}, {"arrows": "to", "color": "red", "from": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 3}, {"arrows": "to", "color": "red", "from": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "to": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "width": 3}, {"arrows": "to", "color": "red", "from": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 3}, {"arrows": "to", "color": "red", "from": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "to": "2deafb11372f085d504db87fd626e478d8e965aa", "width": 3}, {"arrows": "to", "color": "gray", "from": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "to": "2051548f7681c96d603de932ee23406c525276f9", "width": 1}, {"arrows": "to", "color": "gray", "from": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 1}, {"arrows": "to", "color": "gray", "from": "9ae9d2b060e50094be7e473e449f192403019225", "to": "551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5", "width": 1}, {"arrows": "to", "color": "red", "from": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 3}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 1}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "width": 3}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 1}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "2deafb11372f085d504db87fd626e478d8e965aa", "width": 3}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "f2260a53350a5166f757979f435a3c477dec6d3e", "width": 1}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "d8abb8206b913d185b4bd406880131c13759a6ff", "width": 3}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "1a22c4fcff260a8623a1fd677f2e7a7916343dae", "width": 1}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "19c78592bc557b65923ad83e7af26d478721e90f", "width": 3}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "width": 3}, {"arrows": "to", "color": "red", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "c293e24548c22efbb1b856d7c4f53242300df88e", "width": 3}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "45e795ef4736bf2a90b9037d22c40320d478953c", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "2051548f7681c96d603de932ee23406c525276f9", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "3665bec103533566d7580441227f35a000c9b5ff", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 1}, {"arrows": "to", "color": "gray", "from": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "to": "db210fc464d741f3f627190711ec0150d983f280", "width": 1}, {"arrows": "to", "color": "gray", "from": "939043b58d41ca05ca228ae3081c9a540f770a47", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 1}, {"arrows": "to", "color": "gray", "from": "c293e24548c22efbb1b856d7c4f53242300df88e", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 1}, {"arrows": "to", "color": "red", "from": "c293e24548c22efbb1b856d7c4f53242300df88e", "to": "19c78592bc557b65923ad83e7af26d478721e90f", "width": 3}, {"arrows": "to", "color": "gray", "from": "c293e24548c22efbb1b856d7c4f53242300df88e", "to": "3665bec103533566d7580441227f35a000c9b5ff", "width": 1}, {"arrows": "to", "color": "red", "from": "86f260abb52cea53b4dbf3f5c2a5669450983374", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 3}, {"arrows": "to", "color": "red", "from": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "to": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "width": 3}, {"arrows": "to", "color": "gray", "from": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "to": "b539183ab4837cede4ff91180bbce0bc56a2a635", "width": 1}, {"arrows": "to", "color": "gray", "from": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "to": "45e795ef4736bf2a90b9037d22c40320d478953c", "width": 1}, {"arrows": "to", "color": "gray", "from": "c0c807b59e6497fe07de537d9eb11fdbd442ecf6", "to": "db210fc464d741f3f627190711ec0150d983f280", "width": 1}, {"arrows": "to", "color": "gray", "from": "7cdc279cea2d6ffb973bbe71e5e92d5de5ce24bb", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "15bfe54223d28c74f9e39960f7ff1d03167e285f", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "2051548f7681c96d603de932ee23406c525276f9", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "width": 1}, {"arrows": "to", "color": "gray", "from": "d8abb8206b913d185b4bd406880131c13759a6ff", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 1}, {"arrows": "to", "color": "gray", "from": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 1}, {"arrows": "to", "color": "red", "from": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "to": "3b421dcebe21bf45c1930b948cfda7ec95b55ac4", "width": 3}, {"arrows": "to", "color": "gray", "from": "a17fa87dc3eeebff1da725f60ef8a1608eb7986d", "to": "8dfaf780e78545388e527f66b2e58366599c6cd1", "width": 1}, {"arrows": "to", "color": "gray", "from": "6dc740628f9d95bc3d6396f4b3a16b57b9bca4af", "to": "2deafb11372f085d504db87fd626e478d8e965aa", "width": 1}, {"arrows": "to", "color": "gray", "from": "6dc740628f9d95bc3d6396f4b3a16b57b9bca4af", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 1}, {"arrows": "to", "color": "red", "from": "6dc740628f9d95bc3d6396f4b3a16b57b9bca4af", "to": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "width": 3}, {"arrows": "to", "color": "gray", "from": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 1}, {"arrows": "to", "color": "gray", "from": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "to": "15bfe54223d28c74f9e39960f7ff1d03167e285f", "width": 1}, {"arrows": "to", "color": "gray", "from": "19c78592bc557b65923ad83e7af26d478721e90f", "to": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "width": 1}, {"arrows": "to", "color": "red", "from": "19c78592bc557b65923ad83e7af26d478721e90f", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 3}, {"arrows": "to", "color": "red", "from": "19c78592bc557b65923ad83e7af26d478721e90f", "to": "2deafb11372f085d504db87fd626e478d8e965aa", "width": 3}, {"arrows": "to", "color": "gray", "from": "19c78592bc557b65923ad83e7af26d478721e90f", "to": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "width": 1}, {"arrows": "to", "color": "gray", "from": "19c78592bc557b65923ad83e7af26d478721e90f", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "red", "from": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "to": "c2e57a1926217f67a72c617d09fa12ec8e667d0e", "width": 3}, {"arrows": "to", "color": "gray", "from": "62acca98a5bd6ffb3071e9c2a2f559950e4301fc", "to": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "width": 1}, {"arrows": "to", "color": "gray", "from": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 1}, {"arrows": "to", "color": "gray", "from": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 1}, {"arrows": "to", "color": "gray", "from": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "to": "8991f2c72b0a8d5af83f1f3c03e49514d0b20eee", "width": 1}, {"arrows": "to", "color": "gray", "from": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "to": "551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5", "width": 1}, {"arrows": "to", "color": "gray", "from": "be818bd3db1928dcfe24f23ee7afaa09545f9a61", "to": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "width": 1}, {"arrows": "to", "color": "gray", "from": "1a22c4fcff260a8623a1fd677f2e7a7916343dae", "to": "de72c6226ecbc7150dc3e634ca983d001808e6b0", "width": 1}, {"arrows": "to", "color": "red", "from": "4add06e705ea1e6d3179eca1fde473441916c6b2", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "68106f815739c305a6375c9d593a1aac586ee9ed", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "884024110a3ae7050bbf3bde9d737e1fc120eed6", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "f2260a53350a5166f757979f435a3c477dec6d3e", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "551d1f9b0167c19a2f5ade1e5dfedc2f7e7827c5", "width": 3}, {"arrows": "to", "color": "red", "from": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 3}, {"arrows": "to", "color": "red", "from": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 3}, {"arrows": "to", "color": "gray", "from": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 1}, {"arrows": "to", "color": "gray", "from": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "gray", "from": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "to": "19c78592bc557b65923ad83e7af26d478721e90f", "width": 1}, {"arrows": "to", "color": "red", "from": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "to": "32f215640451bb0a06ac97e04c8f89f3dbeb5e15", "width": 3}, {"arrows": "to", "color": "gray", "from": "44202c42e2387c61acd1261d7a750d3f520ca0c7", "to": "3665bec103533566d7580441227f35a000c9b5ff", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "86f260abb52cea53b4dbf3f5c2a5669450983374", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "4add06e705ea1e6d3179eca1fde473441916c6b2", "width": 1}, {"arrows": "to", "color": "red", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "5b7f5488c380cf5085a5dd93e993ad293b225eee", "width": 3}, {"arrows": "to", "color": "red", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "bbae866c5edd477c1c39921436c47fd43d1f6e5c", "width": 3}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "83bc34cf87d07f53b803c878b0e0f78fe6326de1", "width": 1}, {"arrows": "to", "color": "red", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "d61f58df7495c7243252dd1aa43dbf8bf58c9500", "width": 3}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "6664110bc4fee7c79dd1928d2a37dd9222295260", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "ea642fd5aa2a097c249d47ab8687687b42bee6f8", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "f2260a53350a5166f757979f435a3c477dec6d3e", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "57e1a9508f0e0d428b4dfd49d911fae14494778e", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "15bfe54223d28c74f9e39960f7ff1d03167e285f", "width": 1}, {"arrows": "to", "color": "red", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "2051548f7681c96d603de932ee23406c525276f9", "width": 3}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "3848daf38c39983650a3f6ecb4ccd11dbfab757a", "width": 1}, {"arrows": "to", "color": "gray", "from": "2deafb11372f085d504db87fd626e478d8e965aa", "to": "89c0f84361a34c6224c3a1a14ead80ed3c82c9f0", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"layout": {"hierarchical": {"enabled": true, "direction": "UD", "sortMethod": "directed"}}, "physics": {"enabled": false}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    
<script type="text/javascript">
network.on("click", function(params) {
    if (params.nodes.length > 0) {
        var nodeId = params.nodes[0];
        var nodeData = nodes.get(nodeId);
        var titleHtml = nodeData.title || "";
        var abstractHtml = nodeData.abstract ? nodeData.abstract.replace(/\n/g, "<br>") : "";
        var w = window.open("", "_blank", "width=600,height=800,scrollbars=yes");
        if (w) {
            w.document.write("<html><head><title>Paper " + nodeId + "</title></head><body>");
            w.document.write(titleHtml);
            if (abstractHtml) {
                w.document.write("<hr><div><b>Abstract:</b><br>" + abstractHtml + "</div>");
            }
            w.document.write("</body></html>");
            w.document.close();
        } else {
            alert("");
        }
    }
});
</script>
</body>
</html>